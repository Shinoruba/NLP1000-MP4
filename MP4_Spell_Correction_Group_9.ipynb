{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m68PVMPKTRm",
        "outputId": "24cb348c-95ae-4919-819f-cf9e55f114e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Model\n",
        "For the error model: http://norvig.com/ngrams/count_1edit.txt\n",
        "\n",
        "This file contains single-edit spelling correction edits and is based on Peter Norvig's collection of spelling mistakes. The data originates from Wikipedia. The actual words and their misspelling can be found at https://norvig.com/ngrams/spell-errors.txt\n",
        "\n",
        "The file is structured in the following manner: w|c z ; where w is the wrong character, c is the correct, and z is the occurrence of the error\n",
        "\n",
        "For example, in count_1edit.txt, there is 1 occurrence of B|M, where B was substituted M in the example My: By (here the correct word is My but it was misspelled as By).\n",
        "\n",
        "You can normalize the counts based on your language model (i.e. count edit sequences in the corpus)\n",
        "\n",
        "Note: defaultdict subclass was used to count errors without needing to manually check if a key already exists in the dictionary\n",
        "https://www.geeksforgeeks.org/defaultdict-in-python/"
      ],
      "metadata": {
        "id": "dadORC3fJnhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE2c3jBrJfBd",
        "outputId": "0a015368-ffed-4c6e-c9cf-314f151fe893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Model Sample:\n",
            "Error: ('e', 'i'), Count: 917\n",
            "Error: ('a', 'e'), Count: 856\n",
            "Error: ('i', 'e'), Count: 771\n",
            "Error: ('e', 'a'), Count: 749\n",
            "Error: ('a', 'i'), Count: 559\n",
            "Error: ('s', 'c'), Count: 383\n",
            "Error: ('a', 'o'), Count: 353\n",
            "Error: ('o', 'a'), Count: 352\n",
            "Error: ('i', 'a'), Count: 313\n",
            "Error: ('e', 'o'), Count: 295\n"
          ]
        }
      ],
      "source": [
        "filepath = \"count_1edit.txt\"\n",
        "\n",
        "def load_error_model(filepath):\n",
        "    error_model = defaultdict(int)\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            match = re.match(r\"(\\w)\\|(\\w)\\s(\\d+)\", line) # regex to extract errors and their count\n",
        "            if match:\n",
        "                wrong_char, correct_char, count = match.groups()\n",
        "                error_model[(wrong_char, correct_char)] += int(count)\n",
        "    return error_model\n",
        "\n",
        "error_model = load_error_model(filepath)\n",
        "\n",
        "# display 10 lines from the error model\n",
        "print(\"Error Model Sample:\")\n",
        "for i, j in list(error_model.items())[:10]:\n",
        "    print(f\"Error: {i}, Count: {j}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Model\n",
        "For the language model: NLTK's Gutenberg Corpus\n",
        "Information: https://www.nltk.org/book/ch02.htmlLinks\n",
        "\n",
        "This is a walk-through of using and collating all documents within the collection: Using_All_Project_Gutenberg_Documents_in_NLTK.ipynb\n",
        "\n",
        "The walk-through also gives some brief statistics of the corpus\n",
        "\n",
        "Note: The first 3 code blocks below are from the provided walk-through of Gutenberg's documents."
      ],
      "metadata": {
        "id": "yqRXFLPRPVDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_pQmJMKPd0Q",
        "outputId": "906b1fa1-eef0-4cd6-e1de-111d24c238e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting all documents from NLTK's Project Gutenberg Collection...\")\n",
        "all_words = Counter()\n",
        "for filename in nltk.corpus.gutenberg.fileids():\n",
        "  words = [word.lower() for word in nltk.corpus.gutenberg.words(filename)]\n",
        "  all_words.update(words)\n",
        "  print(\"%s; tokens: %d; vocab: %d\" % (filename, len(words), len(set(words))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zajhJWKIPpSZ",
        "outputId": "81fb2016-a16f-4e13-8dbf-de705e45f059"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting all documents from NLTK's Project Gutenberg Collection...\n",
            "austen-emma.txt; tokens: 192427; vocab: 7344\n",
            "austen-persuasion.txt; tokens: 98171; vocab: 5835\n",
            "austen-sense.txt; tokens: 141576; vocab: 6403\n",
            "bible-kjv.txt; tokens: 1010654; vocab: 12767\n",
            "blake-poems.txt; tokens: 8354; vocab: 1535\n",
            "bryant-stories.txt; tokens: 55563; vocab: 3940\n",
            "burgess-busterbrown.txt; tokens: 18963; vocab: 1559\n",
            "carroll-alice.txt; tokens: 34110; vocab: 2636\n",
            "chesterton-ball.txt; tokens: 96996; vocab: 8335\n",
            "chesterton-brown.txt; tokens: 86063; vocab: 7794\n",
            "chesterton-thursday.txt; tokens: 69213; vocab: 6349\n",
            "edgeworth-parents.txt; tokens: 210663; vocab: 8447\n",
            "melville-moby_dick.txt; tokens: 260819; vocab: 17231\n",
            "milton-paradise.txt; tokens: 96825; vocab: 9021\n",
            "shakespeare-caesar.txt; tokens: 25833; vocab: 3032\n",
            "shakespeare-hamlet.txt; tokens: 37360; vocab: 4716\n",
            "shakespeare-macbeth.txt; tokens: 23140; vocab: 3464\n",
            "whitman-leaves.txt; tokens: 154883; vocab: 12452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Overall Statistics\")\n",
        "total_tokens = sum(all_words.values())\n",
        "total_types = len(all_words)\n",
        "\n",
        "print(\"Total tokens: %d\" % total_tokens)\n",
        "print(\"Total vocabulary: %d\" % total_types)\n",
        "print(\"Type/token ratio: %.4f\" % (total_types / total_tokens))\n",
        "print(\"Vocabulary richness: %.4f\" % (total_types / (total_tokens ** (1/2))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNZu5hcPP4Ht",
        "outputId": "596cf6c3-380b-47c6-ee96-a6fbcec49a3a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Statistics\n",
            "Total tokens: 2621613\n",
            "Total vocabulary: 42339\n",
            "Type/token ratio: 0.0161\n",
            "Vocabulary richness: 26.1491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Language Model Formula:\n",
        "count of word (w) / Total tokens"
      ],
      "metadata": {
        "id": "-nMlGhKZRiNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_model = {word: count / total_tokens for word, count in all_words.items()}\n",
        "\n",
        "# print results for observation\n",
        "print(\"\\nLanguage Model (word probabilities):\")\n",
        "for word, prob in list(language_model.items())[10:20]:\n",
        "    print(f\"{word}: {prob:.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpN9Q8zoRzMz",
        "outputId": "3cb8f53d-d05c-4294-922c-c0f5f8dede8f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Language Model (word probabilities):\n",
            "woodhouse: 0.00011939\n",
            ",: 0.07098340\n",
            "handsome: 0.00005035\n",
            "clever: 0.00002975\n",
            "and: 0.03640583\n",
            "rich: 0.00009155\n",
            "with: 0.00671304\n",
            "a: 0.01295386\n",
            "comfortable: 0.00004120\n",
            "home: 0.00025976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the Noisy Channel Model\n",
        "The goal is to implement a model based on probabilities of spelling errors.\n",
        "\n",
        "This is done by generating all 1 edit words from the input word and then query the language model for the most probable word.\n",
        "\n",
        "The vocabulary is limited to 1-grams and candidate words with edit distance of 1 with respect to the input word.\n",
        "\n",
        "NOTE: Error Correction only happens if an error is detected (if word does NOT exist in the Language Model)\n",
        "\n",
        "References:\n",
        "* https://norvig.com/spell-correct.html?fbclid=IwZXh0bgNhZW0CMTEAAR2qMQfJOaK2iDTeJzxv9gIWZKVclmVPdnKAXTr4Up6MV1pvrjz9MgqZVX4_aem_O8k85dDmd9xdgOtKfvEj9g\n",
        "* https://www.geeksforgeeks.org/defaultdict-in-python/"
      ],
      "metadata": {
        "id": "-SxYqmtJ7rEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if word exists in the Language Model.\n",
        "If it exists, no need to perform Spell Correction"
      ],
      "metadata": {
        "id": "aZRCmYOtX57q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_word_correct(word):\n",
        "    return word in language_model\n",
        "\n",
        "is_word_correct('ass')"
      ],
      "metadata": {
        "id": "AXXLUyJ9Xxgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f377a3b9-e8a7-40c6-c999-5ed560c13be6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get all candidate words with Edit Distance of 1"
      ],
      "metadata": {
        "id": "LtjM5hne0YC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Peter Norvig's Spelling Corrector\n",
        "def edits1(word):\n",
        "    # Generate all words with one edit distance from the given word\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [(L + R[1:], \"del\", f\"{R[0]}|-\") for L, R in splits if R]\n",
        "    transposes = [(L + R[1] + R[0] + R[2:], \"tra\", f\"{R[0]+R[1]}|{R[1]+R[0]}\") for L, R in splits if len(R) > 1]\n",
        "    substitutes = [(L + c + R[1:], \"sub\", f\"{R[0]}|{c}\") for L, R in splits if R for c in letters]\n",
        "    inserts = [(L + c + R, \"ins\", f\"-|{c}\") for L, R in splits for c in letters]\n",
        "\n",
        "    return set(deletes + transposes + substitutes + inserts)"
      ],
      "metadata": {
        "id": "-WOuo_UZ5SRw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Error Probability for Candidate Words"
      ],
      "metadata": {
        "id": "_a7zX1pe5oMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error_probability(wrong, correct):\n",
        "    # Get Error probability P(w|c) from Error model (defaultdict)\n",
        "    wrong_char = wrong[0]\n",
        "    correct_char = correct[0]\n",
        "    return error_model.get((wrong_char, correct_char), 1) / sum(error_model.values()) # returns probability of\n",
        "    # Note: 1 is used as a default value in the case that the error isn't recorded in the error model"
      ],
      "metadata": {
        "id": "la1uRBRN6zAC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spell Correction (Noisy Channel Model)"
      ],
      "metadata": {
        "id": "_8fk492W7jft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_correct(word):\n",
        "    if is_word_correct(word):\n",
        "        print(f\"No error: {word} is correctly spelled.\")\n",
        "        return [(word, \"None\", \"-\", language_model[word], \"-\", \"-\")]\n",
        "\n",
        "    candidates = edits1(word) # generate candidates with edit distance of 1\n",
        "    final_candidates = [] # initialize a list to store final candidates\n",
        "\n",
        "    for candidate, edit_type, edit in candidates: # loops for each candidate\n",
        "        if candidate in language_model: # double check to see if the generated candidate word is included in our vocab\n",
        "            prob_c = language_model[candidate]  # P(c)\n",
        "            prob_w_given_c = error_probability(word, candidate)  # P(w|c)\n",
        "            final_prob = prob_c * prob_w_given_c\n",
        "            final_candidates.append((candidate, edit_type, edit, prob_c, prob_w_given_c, final_prob)) # add the results for each candidate to the list\n",
        "\n",
        "    final_candidates.sort(key=itemgetter(5), reverse=True) # sort the final candidates by P(w) * P(w|c)\n",
        "    return final_candidates\n"
      ],
      "metadata": {
        "id": "rIWsr10k9PVN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the Spell Corrector"
      ],
      "metadata": {
        "id": "4TtUFryU-1_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"mother\"  # misspelled word\n",
        "print(\"\\nCorrection Candidates for:\", word)\n",
        "candidates = spell_correct(word)\n",
        "\n",
        "print(f\"\\n{'word':>20}\\t{'candidate':>20}\\tedit_type\\t{'edit':>30}\\t{'P(c)':>12}\\t{'P(w|c)':>12}\\t{'P(c) x P(w|c)':>15}\")\n",
        "for candidate in candidates:\n",
        "    if type(candidate[3]) is str:\n",
        "        format3 = \"{:>12}\".format(\" - \")\n",
        "    else:\n",
        "        format3 = \"{:>12.6f}\".format(candidate[3])\n",
        "\n",
        "    if type(candidate[4]) is str:\n",
        "        format4 = \"{:>12}\".format(\" - \")\n",
        "    else:\n",
        "        format4 = \"{:>12.6f}\".format(candidate[4])\n",
        "\n",
        "    if type(candidate[5]) is str:\n",
        "        format5 = \"{:>15}\".format(\" - \")\n",
        "    else:\n",
        "        format5 = \"{:>15.6e}\".format(candidate[5])\n",
        "\n",
        "    print(f\"{word:>20}\\t{candidate[0]:>20}\\t{candidate[1]:>9}\\t{candidate[2]:>30}\\t{format3}\\t{format4}\\t{format5}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vZlU57g-6TC",
        "outputId": "55c54734-05de-44cf-c1f2-12e90dfb0c54"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correction Candidates for: mother\n",
            "No error: mother is correctly spelled.\n",
            "\n",
            "                word\t           candidate\tedit_type\t                          edit\t        P(c)\t      P(w|c)\t  P(c) x P(w|c)\n",
            "              mother\t              mother\t     None\t                             -\t    0.000463\t          - \t             - \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Other References\n",
        "The following code (Damerau-Levenshtein algorithm) was used as a reference for an earlier implementation, before the group decided to use Norvig's Spelling Corrector.\n",
        "\n",
        "https://stackoverflow.com/questions/44640570/modify-damerau-levenshtein-algorithm-to-track-transformations-insertions-delet?fbclid=IwZXh0bgNhZW0CMTEAAR3flr2AriGp4RHGaswU2hfeen0gyzxB2xWRGbbqN2P1vkdupPSEpQ3ctvc_aem_XImT6tAKsEwjHxLYYXA4pg"
      ],
      "metadata": {
        "id": "UHGWei_bA35Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def levenshtein_distance(string1, string2):\n",
        "    n1 = len(string1)\n",
        "    n2 = len(string2)\n",
        "    return _levenshtein_distance_matrix(string1, string2)[n1, n2]\n",
        "\n",
        "def damerau_levenshtein_distance(string1, string2):\n",
        "    n1 = len(string1)\n",
        "    n2 = len(string2)\n",
        "    return _levenshtein_distance_matrix(string1, string2, True)[n1, n2]\n",
        "\n",
        "def get_ops(string1, string2, is_damerau=False):\n",
        "    i, j = _levenshtein_distance_matrix(string1, string2, is_damerau).shape\n",
        "    i -= 1\n",
        "    j -= 1\n",
        "    ops = list()\n",
        "    while i != -1 and j != -1:\n",
        "        if is_damerau:\n",
        "            if i > 1 and j > 1 and string1[i-1] == string2[j-2] and string1[i-2] == string2[j-1]:\n",
        "                if dist_matrix[i-2, j-2] < dist_matrix[i, j]:\n",
        "                    ops.insert(0, ('transpose', i - 1, i - 2))\n",
        "                    i -= 2\n",
        "                    j -= 2\n",
        "                    continue\n",
        "        index = np.argmin([dist_matrix[i-1, j-1], dist_matrix[i, j-1], dist_matrix[i-1, j]])\n",
        "        if index == 0:\n",
        "            if dist_matrix[i, j] > dist_matrix[i-1, j-1]:\n",
        "                ops.insert(0, ('replace', i - 1, j - 1))\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif index == 1:\n",
        "            ops.insert(0, ('insert', i - 1, j - 1))\n",
        "            j -= 1\n",
        "        elif index == 2:\n",
        "            ops.insert(0, ('delete', i - 1, i - 1))\n",
        "            i -= 1\n",
        "    return ops\n",
        "\n",
        "def execute_ops(ops, string1, string2):\n",
        "    strings = [string1]\n",
        "    string = list(string1)\n",
        "    shift = 0\n",
        "    for op in ops:\n",
        "        i, j = op[1], op[2]\n",
        "        if op[0] == 'delete':\n",
        "            del string[i + shift]\n",
        "            shift -= 1\n",
        "        elif op[0] == 'insert':\n",
        "            string.insert(i + shift + 1, string2[j])\n",
        "            shift += 1\n",
        "        elif op[0] == 'replace':\n",
        "            string[i + shift] = string2[j]\n",
        "        elif op[0] == 'transpose':\n",
        "            string[i + shift], string[j + shift] = string[j + shift], string[i + shift]\n",
        "        strings.append(''.join(string))\n",
        "    return strings\n",
        "\n",
        "def _levenshtein_distance_matrix(string1, string2, is_damerau=False):\n",
        "    n1 = len(string1)\n",
        "    n2 = len(string2)\n",
        "    d = np.zeros((n1 + 1, n2 + 1), dtype=int)\n",
        "    for i in range(n1 + 1):\n",
        "        d[i, 0] = i\n",
        "    for j in range(n2 + 1):\n",
        "        d[0, j] = j\n",
        "    for i in range(n1):\n",
        "        for j in range(n2):\n",
        "            if string1[i] == string2[j]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = 1\n",
        "            d[i+1, j+1] = min(d[i, j+1] + 1, # insert\n",
        "                              d[i+1, j] + 1, # delete\n",
        "                              d[i, j] + cost) # replace\n",
        "            if is_damerau:\n",
        "                if i > 0 and j > 0 and string1[i] == string2[j-1] and string1[i-1] == string2[j]:\n",
        "                    d[i+1, j+1] = min(d[i+1, j+1], d[i-1, j-1] + cost) # transpose\n",
        "    return d\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # GIFTS PROFIT\n",
        "    # FBBDE BCDASD\n",
        "    # SPARTAN PART\n",
        "    # PLASMA ALTRUISM\n",
        "    # REPUBLICAN DEMOCRAT\n",
        "    # PLASMA PLASMA\n",
        "    # FISH IFSH\n",
        "    # STAES STATES\n",
        "    string1 = 'MOTHER'\n",
        "    string2 = 'MOHTER'\n",
        "    for is_damerau in [True, False]:\n",
        "        if is_damerau:\n",
        "            print('=== damerau_levenshtein_distance ===')\n",
        "        else:\n",
        "            print('=== levenshtein_distance ===')\n",
        "        dist_matrix = _levenshtein_distance_matrix(string1, string2, is_damerau=is_damerau)\n",
        "        print(dist_matrix)\n",
        "        ops = get_ops(string1, string2, is_damerau=is_damerau)\n",
        "        print(ops)\n",
        "        res = execute_ops(ops, string1, string2)\n",
        "        print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWYoQWsX0Xim",
        "outputId": "aae6d73b-1d50-4c03-a743-6c216134ed46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== damerau_levenshtein_distance ===\n",
            "[[0 1 2 3 4 5 6]\n",
            " [1 0 1 2 3 4 5]\n",
            " [2 1 0 1 2 3 4]\n",
            " [3 2 1 1 1 2 3]\n",
            " [4 3 2 1 1 2 3]\n",
            " [5 4 3 2 2 1 2]\n",
            " [6 5 4 3 3 2 1]]\n",
            "[('transpose', 3, 2)]\n",
            "['MOTHER', 'MOHTER']\n",
            "=== levenshtein_distance ===\n",
            "[[0 1 2 3 4 5 6]\n",
            " [1 0 1 2 3 4 5]\n",
            " [2 1 0 1 2 3 4]\n",
            " [3 2 1 1 1 2 3]\n",
            " [4 3 2 1 2 2 3]\n",
            " [5 4 3 2 2 2 3]\n",
            " [6 5 4 3 3 3 2]]\n",
            "[('replace', 2, 2), ('replace', 3, 3)]\n",
            "['MOTHER', 'MOHHER', 'MOHTER']\n"
          ]
        }
      ]
    }
  ]
}